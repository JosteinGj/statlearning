\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[10pt,ignorenonframetext,]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
\centering
\begin{beamercolorbox}[sep=16pt,center]{part title}
  \usebeamerfont{part title}\insertpart\par
\end{beamercolorbox}
}
\setbeamertemplate{section page}{
\centering
\begin{beamercolorbox}[sep=12pt,center]{part title}
  \usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
\centering
\begin{beamercolorbox}[sep=8pt,center]{part title}
  \usebeamerfont{subsection title}\insertsubsection\par
\end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{Singapore}
\usefonttheme{serif}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Module 12: Summing up and some cautionary notes},
            pdfauthor={Stefanie Muff, Department of Mathematical Sciences, NTNU},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\newif\ifbibliography
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{multirow}

\title{Module 12: Summing up and some cautionary notes}
\providecommand{\subtitle}[1]{}
\subtitle{TMA4268 Statistical Learning V2020}
\author{Stefanie Muff, Department of Mathematical Sciences, NTNU}
\date{April 17, 2020}

\begin{document}
\frame{\titlepage}

\begin{frame}

Last update: April 17, 2020

\end{frame}

\begin{frame}{Overview}
\protect\hypertarget{overview}{}

\(~\)

\begin{itemize}
\item
  Course content and learning outcome \vspace{2mm}
\item
  Overview of modules and core course topics (with exam type questions)
  \vspace{2mm}
\item
  Some cautionary notes
\end{itemize}

\end{frame}

\begin{frame}

Some of the figures and slides in this presentation are taken (or are
inspired) from G. James et al. (2013).

\end{frame}

\begin{frame}{Learning outcomes of TMA4268}
\protect\hypertarget{learning-outcomes-of-tma4268}{}

\begin{enumerate}
\item
  \textbf{Knowledge.} The student has knowledge about the most popular
  statistical learning models and methods that are used for
  \emph{prediction} and \emph{inference} in science and technology.
  Emphasis is on regression- and classification-type statistical models.
\item
  \textbf{Skills.} The student can, based on an existing data set,
  choose a suitable statistical model, apply sound statistical methods,
  and perform the analyses using statistical software. The student can
  present, interpret and communicate the results from the statistical
  analyses, and knows which conclusions can be drawn from the analyses,
  and what are the caveats.
\end{enumerate}

\(~\)

\textbf{And}: you got to be an expert in using the R language and
writing R Markdown reports.

\end{frame}

\begin{frame}{Core of the course}
\protect\hypertarget{core-of-the-course}{}

Supervised and unsupervised learning:

\begin{itemize}
\tightlist
\item
  \emph{\textcolor{red}{Supervised}}: regression and classification

  \begin{itemize}
  \tightlist
  \item
    examples of regression and classification type problems
  \item
    how complex a model to get the best fit?
    flexiblity/overfitting/underfitting.
  \item
    the bias-variance trade-off
  \item
    how to find the perfect fit - validation and cross-validation (or
    AIC-type solutions)
  \item
    how to compare different solutions
  \item
    how to evaluate the fit - on new unseen data
  \end{itemize}
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \emph{\textcolor{red}{Unsupervised}}: how to find structure or
  groupings in data?
\end{itemize}

and of cause all \textbf{the methods} (with underlying models) to
perform regression, classification and unsupervised learning. We have
gained some theoretical understanding, but in some cases deeper
theoretical background and understanding of the models is provided in
other statistics courses.

\end{frame}

\begin{frame}

\centering

\includegraphics{../../ISLR/Figures/Chapter2/2.7.png} Figure 2.7 from
Gareth James et al. (2013)

\end{frame}

\begin{frame}{The modules}
\protect\hypertarget{the-modules}{}

\begin{block}{1. Introduction}

\(~\)

\begin{itemize}
\tightlist
\item
  Examples, the modules, required background in statistics and
\item
  Introduction to R
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{2. Statistical learning}

\(~\)

\begin{itemize}
\tightlist
\item
  Model complexity

  \begin{itemize}
  \tightlist
  \item
    Prediction vs.~interpretation.\\
  \item
    Parametric vs.~nonparametric.
  \item
    Inflexible vs.~flexible.
  \item
    Overfitting vs.~underfitting
  \end{itemize}
\item
  Supervised vs.~unsupervised.
\item
  Regression and classification.
\item
  Loss functions: quadratic and 0/1 loss.
\item
  Bias-variance trade-off (polynomial example): mean squared error,
  training and test set.
\item
  Vectors and matrices, rules for mean and covariances, the multivariate
  normal distribution.
\item
  Model complexity and the bias-variance trade-off is important in
  ``all'' subsequent modules.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{3. Linear regression}

\(~\)

\begin{itemize}
\tightlist
\item
  The classical normal linear regression model on vector/matrix form.
\item
  Parameter estimators and distribution thereof. Model fit.
\item
  Confidence intervals, hypothesis tests, and interpreting R-output from
  regression.
\item
  Qualitative covariates, interactions.
\item
  This module is a stepping stone for all subsequent uses of regression
  in Modules 6, 7, 8, and 11.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{4. Classification (Mainly two-class problems)}

\(~\)

\begin{itemize}
\tightlist
\item
  Bayes classifier: classify to the most probable class gives the
  minimize the expected 0/1 loss. We usually do not know the probability
  of each class for each input. The Bayes optimal boundary is the
  boundary for the Bayes classifier and the error rate (on a test set)
  for the Bayes classifier is the Bayes error rate.
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  Two paradigms (not in textbook):

  \begin{itemize}
  \tightlist
  \item
    \emph{\textcolor{red}{Diagnostic}} (directly estimating the
    posterior distribution for the classes). Example: KNN classifier,
    logistic regression.
  \item
    \emph{\textcolor{red}{Sampling}} (estimating class prior
    probabilities and class conditional distribution and then putting
    together with Bayes rule). Examples: LDA, QDA with linear or
    quadratic class boundaries.
  \end{itemize}
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  ROC curves, AUC, sensitivity and specificity of classification
  methods.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\includegraphics{12Final_files/figure-beamer/unnamed-chunk-2-1.pdf}

\end{frame}

\begin{frame}

\includegraphics{12Final_files/figure-beamer/unnamed-chunk-3-1.pdf}

\end{frame}

\begin{frame}

Logistic regression gives a probability, given a certain value of the
covariats \(P(Y=1 \, | \, \boldsymbol{x})\).

\includegraphics[width=0.9\linewidth]{12Final_files/figure-beamer/unnamed-chunk-4-1}

\end{frame}

\begin{frame}

\begin{block}{5. Resampling methods}

\(~\)

\textbf{Cros-validation}

\begin{itemize}
\tightlist
\item
  Data rich situation: Training-validation and test set.
\item
  Validation set approach
\item
  Cross-validation for regression and for classification.
\item
  LOOCV, 5 and 10 fold CV
\item
  good and bad issues with validation set, LOOCV, 10-fold CV
\item
  bias and variance for \(k\)-fold cross-validation.
\item
  Selection bias -- the right and wrong way to do cross-validation
\end{itemize}

\(~\)

\textbf{The Bootstrap}

\begin{itemize}
\tightlist
\item
  Idea: Re-use the same data to estimate a statistic of interest by
  \emph{sampling with replacement}.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{6. Linear model selection and regularization:}

\(~\)

Subset-selection. Discriminate:

\begin{itemize}
\tightlist
\item
  \emph{\textcolor{red}{Model selection}}: estimate performance of
  different models to choose the best one.
\item
  \emph{\textcolor{red}{Model assessment}}: having chosen a final model,
  estimate its performance on new data.
\end{itemize}

\(~\)

How?

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Model selection by

  \begin{itemize}
  \tightlist
  \item
    Subset selection (best subset selection or stepwise model selection)
  \item
    Penalizing the training error: AIC, BIC, \(C_p\), Adjusted \(R^2\).
  \item
    Cross-validation.
  \end{itemize}
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Model assessment by

  \begin{itemize}
  \tightlist
  \item
    Cross-validation.
  \end{itemize}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  Shrinkage methods

  \begin{itemize}
  \tightlist
  \item
    ridge regression: quadratic L2 penalty added to RSS
  \item
    lasso regression: absolute L1 penalty added to RSS
  \item
    no penalty on intercept, not scale invariant: center and scale
    covariates
  \end{itemize}

  \(~\)
\item
  Dimension reduction methods:

  \begin{itemize}
  \tightlist
  \item
    principal component analysis: eigenvectors, proportion of variance
    explained, scree plot
  \item
    principal component regression
  \item
    partial least squares
  \end{itemize}

  \(~\)
\item
  High dimensionality issues: multicollinearity, interpretation.
\end{itemize}

\end{frame}

\begin{frame}

\includegraphics{../../ISLR/Figures/Chapter6/6.7.png}

\end{frame}

\begin{frame}

\includegraphics{../../ISLR/Figures/Chapter6/6.4.png}

\end{frame}

\begin{frame}

\includegraphics{../../ISLR/Figures/Chapter6/6.6.png}

\end{frame}

\begin{frame}

\begin{block}{7. Moving beyond linearity}

\(~\)

\begin{itemize}
\item
  Modifications to the multiple linear regression model - when a linear
  model is not the best choice. First look at one covariate, combine in
  ``additive model''.
\item
  Basis functions: fixed functions of the covariates (no parameters to
  estimate).
\item
  Polynomial regression: multiple linear regression with polynomials as
  basis functions.
\item
  Step functions - piece-wise constants. Like our dummy variable coding
  of factors.
\item
  Regression splines: regional polynomials joined smoothly - neat use of
  basis functions. Cubic splines very popular.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{itemize}
\item
  Smoothing splines: smooth functions - minimizing the RSS with an
  additional penalty on the second derivative of the curve. Results in a
  natural cubic spline with knots in the unique values of the covariate.
\item
  Local regressions: smoothed \(K\)-nearest neighbour with local
  regression and weighting. In applied areas \texttt{loess} is very
  popular.
\item
  (Generalized) additive models (GAMs): combine the above. Sum of
  (possibly) non-linear instead of linear functions.
\end{itemize}

\end{frame}

\begin{frame}

\includegraphics{../../ISLR/Figures/Chapter7/7.3.png}

\end{frame}

\begin{frame}

\begin{block}{8. Tree-based methods}

\(~\)

\begin{itemize}
\item
  Method applicable both to regression and classification (\(K\)
  classes) and will give non-linear covariate effects and include
  interactions between covariates.
\item
  A tree can also be seen as a division of the covariate space into
  non-overlapping regions.
\item
  Binary splits using only at the current best split: \emph{greedy
  strategy}.
\item
  Minimization criterion: residual sums of squares (RSS), Gini index or
  cross-entropy.
\item
  Stopping criterion: When to stop: decided stopping criterion - like
  minimal decrease in RSS or less than 10 observations in terminal node.
\item
  Prediction:

  \begin{itemize}
  \tightlist
  \item
    Regression: Mean in box \(R_j\)
  \item
    Classification: Majority vote or cut-off on probabiity.
  \end{itemize}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\tightlist
\item
  \emph{Pruning}: Grow full tree, and then prune back using pruning
  strategy: cost complexity pruning.
\end{itemize}

To improve prediction (but worse interpretation):

\begin{itemize}
\item
  \emph{Bagging} (bootstrap aggregation): draw \(B\) bootstrap samples
  and fit one full tree to each, used the average over all trees for
  prediction.
\item
  \emph{Random forest}: as bagging but only \(m\) (randomly) chosen
  covariates (out of the \(p\)) are available for selection at each
  possible split. Rule of thumb for \(m\) is \(\sqrt{p}\) for
  classificaton and \(p/3\) for regression.
\item
  Out-of-bag estimation can be used for model selection - no need for
  cross-validation.
\item
  Variable importance plots: give the total amount of decrease in RSS or
  Gini index over splits of a predictor - averaged over all trees.
\item
  \emph{Boosting}: fit one tree with \(d\) splits, make residuals and
  fit a new tree, adjust residuals partly with new tree - repeat.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{9. Support vector machines}

\(~\)

\begin{itemize}
\tightlist
\item
  SVM can be used both classification and regression, but we have only
  studied two-class classification.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Aim: find high dimensional hyperplane that separates two classes
  \(f({\bf x})=\beta_0+{\bf x}^T \boldsymbol\beta=0\). If
  \(y_if({\bf x}_i)>0\) observation \({\bf x}_i\) is correctly
  classified.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Central: maximizing the distance (on both sides) from the class
  boundary to the closes observations (the margin \(M\)). This was
  relaxed with slack variables (support vector classifiers), and to
  allow nonlinear functions of \({\bf x}\) by extending an inner product
  to kernels (support vector machine).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Support vectors: observations that lie on the margin or on the wrong
  side of the margin.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  Kernels: generalization of an inner product to allow for non-linear
  boundaries and to speed up calculations due to inner products only
  involve support vectors. Most popular kernel is radial
  \[K(x_i,x_i')=\exp(-\gamma\sum_{j=1}^p (x_{ij}-x_{i'j})^2) \ . \]
\item
  Tuning parameters: cost and parameters in kernels - chosen by CV.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Unfortunately not able to present details since then a course in
  optimization is needed.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Nice connection to non-linar and ridged version of logistic regression
  - comparing hinge loss to logistic loss - but then without the
  computational advanges of the kernel method.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{10. Unsupervised learning}

\(~\)

\begin{itemize}
\tightlist
\item
  Principal component analysis:

  \begin{itemize}
  \tightlist
  \item
    Mathematical details (eigenvectors corresponding to covariance or
    correlation matrix) also in TMA4267.
  \item
    Understanding loadings, scores and the biplot, choosing the number
    of principal components from proportion of variance explained or
    scree-type plots (elbow).
  \end{itemize}
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Clustering:

  \begin{itemize}
  \tightlist
  \item
    \(k\)-means: number of clusters given, iterative algorithm to
    classify to nearest centroid and recalculate centroid
  \item
    hierarchical clustering: choice of distance measure, choice of
    linkage method (single, average, complete),
  \end{itemize}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\centering

\includegraphics[width=0.7\linewidth]{12Final_files/figure-beamer/unnamed-chunk-5-1}

\end{frame}

\begin{frame}

PCA for quality control

\centering

\includegraphics[width=0.8\textwidth,height=\textheight]{PCAforQC.png}

\end{frame}

\begin{frame}

Hierarchical clustering for visualization

\centering

\includegraphics[width=0.7\textwidth,height=\textheight]{../1Intro/heatmap.png}

\end{frame}

\begin{frame}[fragile]

\begin{block}{11. Neural networks}

\(~\)

\begin{itemize}
\item
  Feedforward network architecture: mathematical formula - layers of
  multivariate transformed (\texttt{relu}, \texttt{linear},
  \texttt{sigmoid}) inner products - sequentially connected.
\item
  Loss function to minimize (on output layer): regression (mean
  squared), classification binary (binary crossentropy), classification
  multiple classes (categorical crossentropy)
\item
  Remember the correct choice of output activiation function: mean
  squared loss goes with linear activation, binary crossentropy with
  sigmoid, categorical crossentropy with softmax.
\item
  Gradient based (chain rule) back-propagation - many variants.
\item
  Technicalities: \texttt{nnet} in R
\item
  \texttt{keras} in R. Use of tensors: Piping sequential layers, piping
  to estimation and then to evaluation (metrics).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Some cautionary words}
\protect\hypertarget{some-cautionary-words}{}

\(~\)

\begin{itemize}
\item
  In most of the problems we looked at we could (or had to) choose a set
  of variables to explain or predict an outcome (\(y\)).
\item
  Model selection was the topic of Module 6, but there is more to say
  about it, in particular in the regression context.
\item
  Importantly, the approach to find a model \textbf{heavily depends on
  the aim} for which the model is built.
\end{itemize}

\(~\)

It is important to make the following distinction:

\begin{itemize}
\tightlist
\item
  The aim is to \emph{\textcolor{red}{predict}} future values of \(y\)
  from known regressors.
\item
  The aim is to \emph{\textcolor{red}{explain}} \(y\) using known
  regressors. In this case, the ultimate aim is to find causal
  relationships.
\end{itemize}

\end{frame}

\begin{frame}

\(\rightarrow\) Even among statisticians there is no real consensus
about how, if, or when to select a model:

\vspace{2mm}

\includegraphics[width=0.9\textwidth,height=\textheight]{graphics/brewer_title.png}
\includegraphics[width=0.9\textwidth,height=\textheight]{graphics/brewer.png}

Note: The first sentence of a paper in \emph{Methods in Ecology and
Evolution} from 2016 is: ``Model selection is difficult.''

\end{frame}

\begin{frame}

\begin{block}{Why is finding a model so hard?}

\(~\)

A model is an approximation of the reality. The aim of statistics and
data analysis is to find connections (explanations or predictions)
thanks to simplifications of the real world.

\(~\)

Box (1979):
\emph{\textcolor{red}{``All models are wrong, but some are useful.''}}

\(~\) \(~\)

\(\rightarrow\) There is often not a ``right'' or a ``wrong'' model --
but there are more and less useful ones.

\(~\)

\(\rightarrow\) Finding a model or the appropriate method with good
properties is sometimes an art\ldots{}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Predictive and explanatory models}

\(~\)

When choosing a method or a model, you need to be clear about the scope:

\(~\)

\begin{itemize}
\tightlist
\item
  \textbf{\textcolor{red}{Predictive models}}: These are models that aim
  to predict the outcome of future subjects.\\
  \textbf{Example:} In the bodyfat example (module 3) the aim is to
  predict people's bodyfat from factors that are easy to measure (age,
  BMI, weight,..).
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  \textbf{\textcolor{red}{Explanatory models}}: These are models that
  aim at understanding the (causal) relationship between covariates and
  the response.\\
  \textbf{Example:} The South African heart disease data aims to
  identify important risk factors for coronary heart disease.
\end{itemize}

\(~\)

\(\rightarrow\) \textbf{The model selection strategy depends on this
distinction.}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Prediction vs explanation}

\(~\)

When the aim is \textbf{\emph{prediction}}, the best model is the one
that best predicts the fate of a future subject (smallest test error
rate). This is a well defined task and ``objective'' variable selection
strategies to find the model which is best in this sense are potentially
useful.

\(~\)

However, when used for \textbf{\emph{explanation}} the best model will
depend on the scientific question being asked, \textbf{and automatic
variable selection strategies have no place}.

\(~\)

\scriptsize

Chapters 27.1 and 27.2 in Clayton and Hills (1993)

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Model selection with AIC, BIC, \(C_p\), adjusted \(R^2\)}

\(~\)

Given \(m\) potential variables to be included in a model. Remember from
Module 6: \vspace{2mm}

\begin{itemize}
\item
  Subset selection using forward, backward or best subset selection
  method.
\item
  Use an ``objective'' criterion to find the ``best'' model.
\end{itemize}

\(~\)

\textbf{Cautionary Note}:

The coefficients of such an optimized ``best'' model should \emph{not be
interpreted} in a causal sense! Why?

\(~\)

\(\rightarrow\) Subset selection may lead to \textbf{biased parameter
estimates}, thus \textbf{do not draw (biological, medical,..)
conclusions} from models that were optimized for prediction, for example
by AIC/AICc/BIC minimization!

\vspace{1mm}
\scriptsize

See, e.g., Freedman (1983), Copas (1983)

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Your aim is prediction?}

\(~\)

Then you are basically free to do what you want. Your only aim is to
minimize some sort of prediction error.

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Your aim is explanation?}

\(~\)

\emph{Explanation} means that you will want to interpret the regression
coefficients, 95\% CIs and \(p\)-values. It is then often assumed that
some sort of causality (\(x\rightarrow y\)) exists.

\(~\)

In such a situation, you should formulate a
\emph{\textcolor{red}{confirmatory model}}:

\(~\)

\begin{itemize}
\item
  Start with a \emph{\textcolor{red}{clear hypothesis}}. \vspace{2mm}
\item
  Select your covariates according to
  \emph{\textcolor{red}{a priori knowledge}}. \vspace{2mm}
\item
  Ideally, formulate \emph{\textcolor{red}{only one}} or a few model(s)
  \emph{\textcolor{red}{before you start analysing your data}}.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Explanatory: Confirmatory vs exploratory}

\(~\)

Any \textbf{additional analyses} that you potentially do with your data
have the character of \textbf{exploratory models}.

\(~\)

\(\rightarrow\) Two sorts of
\emph{\textcolor{red}{explanatory models/analyses}}:

\(~\)

\begin{itemize}
\tightlist
\item
  \textcolor{red}{Confirmatory}:

  \begin{itemize}
  \tightlist
  \item
    Clear hypothesis and \emph{\bf a priori} selection of regressors for
    \(y\).
  \item
    \textbf{No subset selection!}
  \item
    Allowed to interpret the results and draw quantitative conclusions.
  \end{itemize}
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  \textcolor{red}{Exploratory}:

  \begin{itemize}
  \tightlist
  \item
    Build whatever model you want, but the results should only be used
    to generate new hypotheses, a.k.a. ``speculations''.
  \item
    Clearly label the results as ``exploratory''.
  \end{itemize}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

Explanatory models to confirm hypotheses (so-called confirmatory models)
have a long tradition medicine. In fact, the main conclusions in a study
are only allowed to be drawn from the main model (which needs to be
specified even before data are collected):

\(~\)

\includegraphics[width=10.5cm]{graphics/claytonHills.jpg} Clayton and
Hills (1993)

\end{frame}

\begin{frame}

\begin{block}{Interpretation of exploratory models?}

\(~\)

Results from exploratory models can be used to generate new hypotheses,
but it is then \emph{not allowed to draw causal conclusions from them},
or to over-interpret effect-sizes.

\(~\)

\(\rightarrow\) In biological publications it is (unfortunately) still
common practice that exploratory models, which were optimized with model
selection criteria (like AIC), are used to draw conclusions as if the
models were confirmatory.

\(~\)

\(\rightarrow\) We illustrate why this is a problem on the next slides.

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Illustration: Model selection bias}

\(~\)

\textbf{Aim of the example:} To illustrate how model selection purely
based on AIC can lead to biased parameters and overestimated effects.

\(~\)

\emph{Procedure:} \vspace{2mm}

1.) Randomly generate 100 data points for 50 covariables
\(x^{(1)},\ldots, x^{(50)}\) and a response \(y\):

\(~\) \scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{51} \OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{51}\NormalTok{))}
\KeywordTok{names}\NormalTok{(data)[}\DecValTok{51}\NormalTok{] <-}\StringTok{ "Y"}
\end{Highlighting}
\end{Shaded}

\(~\)

\normalsize

\texttt{data} is a 100\(\times\) 51 matrix, where the last column is the
response. The \textbf{data were generated completely independently}, the
covariates do not have any explanatory power for the response!

\end{block}

\end{frame}

\begin{frame}[fragile]

2.) Fit a linear regression model of \(y\) against all the 50 variables
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i^{(1)} + \ldots + \beta_{50}x_i^{(50)} + \epsilon_i \ .
\end{equation*}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r.lm <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{., data)}
\end{Highlighting}
\end{Shaded}

\normalsize

As expected, the distribution of the \(p\)-values is (more or less)
uniform between 0 and 1, with none below 0.05:

\scriptsize
\center

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(}\KeywordTok{summary}\NormalTok{(r.lm)}\OperatorTok{$}\NormalTok{coef[}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{], }\DataTypeTok{freq =}\NormalTok{ T, }\DataTypeTok{main =} \StringTok{"50 variables"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"p-values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4cm]{12Final_files/figure-beamer/unnamed-chunk-8-1}

\end{frame}

\begin{frame}[fragile]

3.) Then use AICc minimization to obtain the objectively ``best'' model:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\NormalTok{r.AICmin <-}\StringTok{ }\KeywordTok{stepAIC}\NormalTok{(r.lm, }\DataTypeTok{direction =} \KeywordTok{c}\NormalTok{(}\StringTok{"both"}\NormalTok{), }\DataTypeTok{trace =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{AICc =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\centering

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(}\KeywordTok{summary}\NormalTok{(r.AICmin)}\OperatorTok{$}\NormalTok{coef[}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{], }\DataTypeTok{freq =}\NormalTok{ T, }\DataTypeTok{main =} \StringTok{"18 variables of minimal AICc model"}\NormalTok{, }
    \DataTypeTok{xlab =} \StringTok{"p-values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=5cm]{12Final_files/figure-beamer/pvaluesHist-1}

\flushleft
\normalsize

The distribution of the \(p\)-values is now skewed: many of them reach
rather small values (1 have \(p<0.05\)). This happened \emph{although
none of the variables has any explanatory power!}

\end{frame}

\begin{frame}{After TMA4268 - what is next?}
\protect\hypertarget{after-tma4268---what-is-next}{}

What are the statistical challenges we have not covered?

Do you want to learn more about the methods we have looked at in this
course? And also methods that are more tailored towards specific types
of data? Then we have many statistics courses that you may choose from.

An overview of statistics courses is kindly put together by Mette
Langaas: \url{https://folk.ntnu.no/mettela/Talks/3klinfo20190325.html}

\end{frame}

\begin{frame}{Final word}
\protect\hypertarget{final-word}{}

\textbf{Thank you for attending this course - good luck for the
compulsory exercise 3 and let's hope we are soon back into normal
teaching mode in the post-Covid-19 era!}

\end{frame}

\begin{frame}{References}
\protect\hypertarget{references}{}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-clayton.hills1993}{}%
Clayton, D., and M. Hills. 1993. \emph{Statistical Models in
Epidemiology}. Oxford: Oxford University Press.

\leavevmode\hypertarget{ref-copas1983}{}%
Copas, J. B. 1983. ``Regression, Prediction and Shrinkage.''
\emph{Journal of the Royal Statistical Society. Series B (Statistical
Methodology)} 45: 311--54.

\leavevmode\hypertarget{ref-freedman1983}{}%
Freedman, D. A. 1983. ``A Note on Screening Regression Equations.''
\emph{The American Statistician} 37: 152--55.

\leavevmode\hypertarget{ref-ISL}{}%
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
2013. \emph{An Introduction to Statistical Learning}. Vol. 112.
Springer.

\leavevmode\hypertarget{ref-james.etal}{}%
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. \emph{An
Introduction to Statistical Learning with Applications in R}. New York:
Springer.

\end{frame}

\end{document}
