\documentclass[ignorenonframetext,]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{Singapore}
\usefonttheme{serif}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\newif\ifbibliography
\hypersetup{
            pdftitle={Module 2, Part 1: Statistical Learning},
            pdfauthor={Stefanie Muff, Department of Mathematical Sciences, NTNU},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{caption}
% These lines are needed to make table captions work with longtable:
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight0.8\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \let\insertsectionnumber\relax
    \let\sectionname\relax
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \let\insertsubsectionnumber\relax
  \let\subsectionname\relax
  \frame{\subsectionpage}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
\usepackage{xcolor}

\title{Module 2, Part 1: Statistical Learning}
\subtitle{TMA4268 Statistical Learning V2020}
\author{Stefanie Muff, Department of Mathematical Sciences, NTNU}
\date{January xx, 2020}

\begin{document}
\frame{\titlepage}

\begin{frame}{Introduction}

\begin{block}{Learning material for this module}

\begin{itemize}
\tightlist
\item
  James et al (2013): An Introduction to Statistical Learning. Chapter
  2.\\
\item
  Additional material (in this module page) on random variables,
  covariance matrix and the multivariate normal distribution (known for
  students who have taken TMA4267 Linear statistical models).
\end{itemize}

Some of the figures and slides in this presentation are taken (or are
inspired) from ``An Introduction to Statistical Learning, with
applications in R'' (Springer, 2013) with permission from the authors:
G. James, D. Witten, T. Hastie and R. Tibshirani.

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What will you learn?}

\begin{itemize}
\tightlist
\item
  Statistical learning and examples thereof
\item
  Introduce relevant notation and terminology.
\item
  Prediction accuracy vs.~model interpretability
\item
  Bias-variance trade-off
\item
  Classification (a first look):

  \begin{itemize}
  \tightlist
  \item
    The Bayes classifier
  \item
    K nearest neighbour (KNN) classifier
  \end{itemize}
\item
  The basics of random vectors, covariance matrix and the multivariate
  normal distribution.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{What is statistical learning?}

\emph{Statistical learning} is the process of learning from data.

By applying \emph{statistical methods} on a \emph{data set} (called the
\emph{training set}), we would like to \emph{draw conclusions} about the
relations between the variables (aka \emph{inference}) or to \emph{find
a predictive function} (aka \emph{predition}) for new observations.

Also, we would like to find structures in the data that help us to learn
something about the real world.

Statistical learning plays a key role in many areas of science, finance
and industry.

\end{frame}

\begin{frame}

\begin{block}{Variable types}

Variables can be characterized as either \emph{quantitative} or
\emph{qualitative}.

\textbf{Quantitative} variables are variables from a continuous set,
they have a numerical value.

\begin{itemize}
\tightlist
\item
  Examples: a person's weight, a company's income, the age of a
  building, the temperature outside, the level of precipitation etc.
\end{itemize}

\textbf{Qualitative} variables are variables from a discrete set, from a
set of \(K\) different classes/labels/categories.

\begin{itemize}
\item
  Examples: type of fruit \{apples, oranges, bananas, \ldots{}\}, sex
  \{male, female, other \}, education level.
\item
  Qualitative variables which have only two classes are called
  \emph{binary} variables and are usually coded by 0 (no) and 1 (yes).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Examples of learning problems}

\begin{itemize}
\item
  To predict the price of a stock 3 months from now, based on company
  performance measures and economic data. Here the response variable is
  quantitative (price).
\item
  Spam detection for emails.
\item
  To identify the risk factors for Prostate cancer.
\item
  To predict whether someone will suffer from a heart disease, given
  knowledge about condition, behaviour, age etc.
\item
  To predict whether someone will suffer a heart attack on the basis of
  demographic, diet and clinical measurements. Here the outcome is
  binary ({yes},{no}) with both qualitative and quantitative input
  variables.
\item
  Predict soemeone's body fat, given BMI, weight, age, etc.
\end{itemize}

\end{frame}

\begin{frame}

South African coronary heart disease data: 462 observations and 10
variables.

\begin{center}\includegraphics[width=16cm]{2StatLearn.1_files/figure-beamer/heart-1} \end{center}

\end{frame}

\begin{frame}

\begin{block}{Handwritten digit recognition:}

To identify the numbers in a handwritten ZIP code, from a digitized
image. This is a classification problem, where the response variable is
categorical with classes \{0, 1, 2, \ldots{}, 9\} and the task is to
correctly predict the class membership.

\includegraphics{digits.png}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Email classification (spam detection):}

The goal is to build a spam filter. This filter can based on the
frequencies of words and characters in emails. The table below show the
average percentage of words or characters in an email message, based on
4601 emails of which 1813 were classified as a spam.

\begin{longtable}[]{@{}lrrrrrr@{}}
\toprule
& you & free & george & ! & \$ & edu\tabularnewline
\midrule
\endhead
not spam & 1.27 & 0.07 & 1.27 & 0.11 & 0.01 & 0.29\tabularnewline
spam & 2.26 & 0.52 & 0.00 & 0.51 & 0.17 & 0.01\tabularnewline
\bottomrule
\end{longtable}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What makes a Nobel Prize winner?}

Perseverance, luck, skilled mentors or simply chocolate consumption? An
article published in the New England Journal of Medicine have concluded
with the following:

\begin{quote}
Chocolate consumption enhances cognitive function, which is a sine qua
non for winning the Nobel Prize, and it closely correlates with the
number of Nobel laureates in each country. It remains to be determined
whether the consumption of chocolate is the underlying mechanism for the
observed association with improved cognitive function.
\end{quote}

The figure shows the number of Nobel Laureates per 10 million population
against countries' annual per capita chocolate consumption.

You can read the article
\href{http://www.nejm.org/doi/full/10.1056/NEJMon1211064}{here} and a
informal review of the article
\href{https://blogs.scientificamerican.com/the-curious-wavefunction/chocolate-consumption-and-nobel-prizes-a-bizarre-juxtaposition-if-there-ever-was-one/}{here}.

\end{block}

\end{frame}

\begin{frame}

\includegraphics{chocolate.jpeg}

\end{frame}

\begin{frame}

\begin{block}{Were there common underlying aims and elements of these
examples of statistical learning?}

\begin{itemize}
\tightlist
\item
  To predict the price of a stock 3 months from now, based on company
  performance measures and economic data.
\item
  To identify the risk factors for developing diabetes based on diet,
  physical activity, family history and body measurements.
\item
  The goal is to build a spam filter.
\item
  To predict whether someone will suffer a heart attack on the basis of
  demographic, diet and clinical measurements.
\item
  To identify the numbers in a handwritten ZIP code, from a digitized
  image.
\item
  What makes a Nobel Prize winner? Perseverance, luck, skilled mentors
  or simply chocolate consumption?
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{The Supervised Learning Problem}

\textcolor{blue}{\emph{Starting point}}:

\begin{itemize}
\item
  Outcome measurement \(Y\) (also called dependent variable, response,
  target).
\item
  Vector of \(p\) predictor measurements \emph{X} (also called inputs,
  regressors, covariates, features, independent variables).
\item
  In the \textbf{regression problem}, \(Y\) is quantitative (e.g price,
  blood pressure).
\item
  In the \textbf{classification problem}, \(Y\) takes values in a
  finite, unordered set (survived/died, digit 0-9, cancer class of
  tissue sample).
\item
  We have training data \((x_1, y_1), \ldots , (x_N , y_N )\). These are
  observations (examples, instances) of these measurements.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Supervised learning and its objectives}

Our data set (training set) consists of \(n\) measurement of the
response variable \(Y\) and of \(p\) covariates \(x\):
\[(y_1, x_{11}, x_{12},\ldots, x_{1p}), (y_2, x_{21},\ldots, x_{2p}), \ldots, (y_n, x_{n1}, x_{n2},\ldots, x_{np}).\]
\(~\)

On the basis of the \emph{training data} we would like to:

\begin{itemize}
\item
  \textbf{Accurately predict} unseen test cases.
\item
  \textbf{Understand} which input affects the outcomes, and how.
\item
  \textbf{Assess the quality} of your predictions and inference.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

Supervised learning examples (we will study):

\begin{itemize}
\tightlist
\item
  Linear regression (M3), Logistic regression (M4), Generalized additive
  models (M7)
\item
  Classification trees, bagging, boosting (M8), K-nearest neighbor
  classifier (M2, M4)
\item
  Support vector machines (M9)
\end{itemize}

\end{frame}

\begin{frame}{The Unsupervised Learning Problem}

\begin{itemize}
\item
  There is \textbf{no outcome variable} \(y\), just a set of predictors
  (features) \(x_i\) measured on a set of samples.
\item
  Objective is more fuzzy -- find (hidden) patterns or groupings in the
  data - in order to \emph{gain insight and understanding}. There is no
  \emph{correct} answer.
\item
  Difficult to know how well your are doing.
\item
  Different from supervised learning, but can be useful as a
  pre-processing step for supervised learning.
\end{itemize}

Examples in the course:

\begin{itemize}
\tightlist
\item
  Clustering (M10)
\item
  Principal component analysis (M10)
\end{itemize}

\end{frame}

\begin{frame}{Semi-supervised learning}

(Not considered in this course)

\begin{itemize}
\item
  Our data set consists of a input data, and some of the data has
  labelled responses.
\item
  This situation can for example occur if the measurement of input data
  is cheap, while the output data is expensive to collect.
\item
  Classical solutions (likelihood-based) to this problem exists in
  statistics (missing at random observations).
\end{itemize}

\end{frame}

\begin{frame}{Task}

Find examples to explain the difference between supervised and
unsupervised learning.

\begin{itemize}
\item
\item
\item
\end{itemize}

\end{frame}

\begin{frame}{Overall philosophy}

It is important to understand the ideas behind the various techniques,
in order to know how and when to use them.

\begin{itemize}
\item
  One has to understand the simpler methods first, in order to grasp the
  more sophisticated ones.
\item
  It is important to accurately assess the performance of a method, to
  know how well or how badly it is working
\end{itemize}

\(\rightarrow\) \textbf{Simpler methods often perform as well as fancier
ones!}

\begin{itemize}
\item
  This is an exciting research area, having important applications in
  science, industry and finance.
\item
  Statistical learning is a fundamental ingredient in the training of a
  modern data scientist.
\end{itemize}

\end{frame}

\begin{frame}{Statistical Learning vs.~Machine Learning}

\begin{itemize}
\item
  Machine learning arose as a subfield of Artificial Intelligence.
\item
  Statistical learning arose as a subfield of Statistics.
\item
  There is much overlap --- both fields focus on supervised and
  unsupervised problems:

  \begin{itemize}
  \tightlist
  \item
    Machine learning has a greater emphasis on large scale applications
    and prediction accuracy.
  \item
    Statistical learning emphasizes models and their interpretability,
    and precision and uncertainty.
  \end{itemize}
\item
  The distinction has become more and more blurred, and there is a great
  deal of ``cross-fertilization''.
\item
  Machine learning has the upper hand in Marketing!
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  There is a controversy and some scepticism against ``too fancy'' ML
  methods.
\item
  Criticism: ML often re-invents existing methods and names them
  differently, but often without awareness of existing methods in
  statistics.
\item
  Almost weekly new literature that delivers comparison. Often, the
  ``simple'' statistcal methods ``win''.
\end{itemize}

\includegraphics{ML_vs_stat.png}

\end{frame}

\begin{frame}

\begin{block}{What is the aim in statistical learning?}

Assume:

\begin{itemize}
\tightlist
\item
  we observe one \emph{quantitative} response \(Y\) and
\item
  \(p\) different predictors \(x_1, x_2,... , x_p\).
\end{itemize}

We assume that there is a function \(f\) that relates the response and
the predictor variables: \[ Y = f(x) + \varepsilon,\] where
\(\varepsilon\) is a random error term with mean 0 and independent of
\(x\).

\centering
{\bf The aim is to estimate $f$.}

\end{block}

\end{frame}

\begin{frame}{Example 1}

Sales of a product, given advertising budgets in different media.

\includegraphics{../../ISLR/Figures/Chapter2/2.1.png}

\end{frame}

\begin{frame}{Example 2}

Income for given levels of education.

\includegraphics{../../ISLR/Figures/Chapter2/2.2.png}

\end{frame}

\begin{frame}

There are two main reasons for estimating \(f\):

\begin{itemize}
\item
  \textbf{Prediction}
\item
  \textbf{Inference}
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Reason 1: Prediction}

\(~\)

\textbf{Aim}: predict a response \(Y\) given new observations \(x\) of
the covariates as accurately as possible.

\(~\) Notation:

\[\hat{Y} = \hat{f}(x).\]

\begin{itemize}
\item
  \(\hat{f}\): estimated \(f\)
\item
  \(\hat{Y}\) prediction for \(Y\) given \(x\).
\item
  We do not really care about the shape of \(f\) (``black box'').
  \(\rightarrow\) no interpretation of regression parameters when the
  aim is purely prediction!
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

There are two quantities which influence the accuracy of \(\hat{Y}\) as
a prediction of \(Y\):

\begin{itemize}
\tightlist
\item
  The \emph{reducible error} has to do with our estimate \(\hat{f}\) of
  \(f\). This error can be reduced by using the most \emph{appropriate}
  statistical learning technique.
\item
  The \emph{irreducible error} comes from the error term \(\varepsilon\)
  and cannot be reduced by improving \(f\). This is related to the
  unobserved quantities influencing the response and possibly the
  randomness of the situation.
\end{itemize}

For a given \(\hat{f}\) and a set of predictors \(X\) which gives
\(\hat{Y}=\hat{f}(X)\), we have

\[\text{E}[(Y-\hat{Y})^2] = \underbrace{\text{E}[(f(X)-\hat{f}(X))^2]}_{reducible} + \underbrace{Var(\epsilon)}_{irreducible}\]

\end{frame}

\begin{frame}

\begin{block}{Q: If there were a \emph{deterministic} relationship
between the response and a set of predictors, would there then be both
reducible and irreducible error?}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Reason 2: Inference}

\(~\)

\textbf{Aim}: \emph{understand} how the response variable is affected by
the various predictors (covariates).

\(~\)

The \emph{exact form} of \(\hat{f}\) is of \emph{main interest}.

\begin{itemize}
\tightlist
\item
  Which predictors are associated with the response?
\item
  What is the relationship between the response and each predictor?
\item
  Can the relationship be linear, or is a more complex model needed?
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Estimating \(f\)}

Overall idea:

\begin{itemize}
\tightlist
\item
  Using available \emph{training data} \((x_1,y_n),\ldots (x_n,y_n)\) to
  estimate \(\hat{f}\), such that \(Y\approx \hat{f}(X)\) for any
  \((X,Y)\) (also those that have not yet been observed).
\end{itemize}

Two main approaches:

\begin{itemize}
\tightlist
\item
  Parametric methods
\item
  Non-parametric methods
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Parametric methods}

\(~\)

Asumption about the form or shape of the function \(f\).

The multiple linear model (M3) is an example of a parametric method:

\[f(x) = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p+\varepsilon, \]

with \(\varepsilon \sim N(0,\sigma^2)\).

\(~\)

The task simplifies to finding estimates of the \(p+1\) coefficients
\(\beta_0, \beta_1, .. ,\beta_p\). To do this we use the training data
to fit the model, such that
\[Y \approx \hat{\beta_0} + \hat{\beta_1} x_1 + ... + \hat{\beta_p} x_p.\]

\end{block}

\end{frame}

\begin{frame}

Fitting a parametric models is thus done in two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select a form for the function \(f\).\\
\item
  Estimate the unknown parameters in \(f\) using the training set.
\end{enumerate}

\end{frame}

\begin{frame}

\begin{block}{Non-parametric methods}

\(~\)

\begin{itemize}
\item
  Non-parametric methods seek an estimate of \(f\) that gets close to
  the data points, but without making explicit assumptions about the
  form of the function \(f\).
\item
  The \(K\)-nearest neighbour algorithm is an example of a
  non-parametric model. Used in classification, this algorithm predicts
  a class membership for a new observation by making a majority vote
  based on its \(K\) nearest neighbours. We will discuss the
  \(K\)-nearest neighbour algorithm later in this module.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Q: What are advantages and disadvantages of parametric and
non-parametric methods?}

Hints: interpretability, amount of data needed, complexity, assumptions
made, prediction accuracy, computational complexity, over/under-fit.

\end{block}

\end{frame}

\begin{frame}

\begin{block}{A: Parametric methods}

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.47\columnwidth}\raggedright\strut
Advantages\strut
\end{minipage} & \begin{minipage}[b]{0.47\columnwidth}\raggedright\strut
Disadvantages\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Simple to use and easy to understand\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
The function \(f\) is constrained to the specified
form.\vspace{2mm}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Requires little training data\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
The assumed function form of \(f\) will in general not match the true
function, potentially giving a poor estimate.\vspace{2mm}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Computationally cheap\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Limited flexibility\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{A: Non-parametric methods}

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.47\columnwidth}\raggedright\strut
Advantages\strut
\end{minipage} & \begin{minipage}[b]{0.47\columnwidth}\raggedright\strut
Disadvantages\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Flexible: a large number of functional forms can be fitted\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Can overfit the data\vspace{2mm}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
No strong assumptions about the underlying function are made\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Computationally more expensive as more parameters need to be
estimated\vspace{2mm}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Can often give good predictions\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright\strut
Much data is required to estimate (the complex) \(f\).\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\end{block}

\end{frame}

\begin{frame}{Prediction accuracy vs.~interpretability}

(we are warming up to the bias--variance trade--off)

\textbf{Inflexible} methods:

\begin{itemize}
\tightlist
\item
  Linear regression (M3)
\item
  Linear discriminant analysis (M4)
\item
  Subset selection and lasso (M6)
\end{itemize}

\textbf{Flexible} methods:

\begin{itemize}
\tightlist
\item
  KNN classification (M2, M4), KNN regression, Smoothing splines (M7)
\item
  Bagging and boosting (M8), support vector machines (M9)
\item
  Neural networks (M11)
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Why would I ever prefer an inflexible method?}

\(~\)

Example: Prediction of icome from ``Years of Education'' and
``Seniority''\vspace{2mm}

\centering
\includegraphics[width=0.45000\textwidth]{../../ISLR/Figures/Chapter2/2.4.png}
\includegraphics[width=0.45000\textwidth]{../../ISLR/Figures/Chapter2/2.6.png}

\end{block}

\end{frame}

\begin{frame}

The choice of a flexible or inflexible method depends on the goal in
mind.

\begin{itemize}
\tightlist
\item
  For \textbf{inference} an inflexible model is easier to interpret.
\item
  For \textbf{prediction} a flexible model is more powerful.
\end{itemize}

\textbf{Overfitting} occurs when the estimated function \(f\) is too
closely fit to the observed data points.

\textbf{Underfitting} occurs when the estimated function \(f\) is too
rigid to capture the underlying structure of the data.

We illustrate this by a toy example using polynomial regression.

\end{frame}

\begin{frame}

\begin{block}{Polynomial regression example (simulation)}

Consider a covariate \(x\) observed between -2 to 4 and \(n=61\)
observations.

\begin{center}\includegraphics{2StatLearn.1_files/figure-beamer/data-1} \end{center}

\end{block}

\end{frame}

\begin{frame}

We impose a relationship between reponse \(Y\) and covariate \(x\):

\[ Y=x^2 + \varepsilon\] with error (noise) term
\(\varepsilon\sim N(0,\sigma^2)\) with \(\sigma=2\). It is a substitue
for all the unobserved variables that are not in our equation, but that
might influence \(Y\).

We call \(Y=x^2\) the \emph{truth}.

\end{frame}

\begin{frame}

\begin{center}\includegraphics{2StatLearn.1_files/figure-beamer/truth-1} \end{center}

\end{frame}

\begin{frame}

Next, we want to fit a function to the observations \emph{without}
knowing the true relationship, and we have tried different parametric
polynomial functions.

\begin{itemize}
\item
  \textbf{poly1}: Simple linear model of the form \(\beta_0+\beta_1 x\)
  fitted to the observations. \emph{Underfits} the data.
\item
  \textbf{poly2}: Quadratic polynomial fit to the data, of the form
  \(\beta_0+\beta_1 x +\beta_2 x^2\). This fits well.
\item
  \textbf{poly10}: Polynomial of degree 10 fit of the form
  \(\beta_0+\beta_1 x +\beta_2 x^2+\cdots +\beta_{10}x^{10}\)
  \emph{Overfits} the data.
\item
  \textbf{poly20}: Polynomial of degree 10 fit of the form
  \(\beta_0+\beta_1 x +\beta_2 x^2+\cdots +\beta_{20}x^{20}\)
  \emph{Overfits} the data.
\end{itemize}

We will discuss polynomial regression in M7.

\end{frame}

\begin{frame}

\begin{center}\includegraphics{2StatLearn.1_files/figure-beamer/overunderfit-1} \end{center}

The degree of the polynomial is a \emph{flexibility parameter}.

\end{frame}

\begin{frame}

We can now ask:

\begin{itemize}
\item
  Which of these models performs ``best''?
\item
  Is there \emph{one} method that dominates all others?
\end{itemize}

\end{frame}

\begin{frame}{Assessing model accuracy}

\textbf{No method} dominates all others over all possible data sets.

\begin{itemize}
\tightlist
\item
  That is why we need to learn about many different methods.
\item
  For a given data set we need to know how to decide which method
  produces the \emph{best} results.
\item
  We need to understand what \emph{best} means.
\item
  How close is the predicted response to the true response value?
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Measuring the Quality of Fit}

\begin{itemize}
\item
  A popular measure for the quality of fit: \emph{Training MSE} (mean
  squared error).
\item
  It is the mean of squared differences between prediction and truth for
  the training data (the same values that were used to estimate \(f\)):
\end{itemize}

\[ \text{MSE}_{\text{train}}=\frac{1}{n}\sum_{i=1}^n (y_i-\hat{f}(x_i))^2\]

\begin{itemize}
\tightlist
\item
  But: we are \emph{not} interested in how the method works on the
  training data. We want to know how good the method is when we use it
  on \emph{previously unseen test data} (e.g., future data).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

Examples:

\begin{itemize}
\item
  We don't want to predict last weeks stock price, we want to predict
  the stock price next week.
\item
  We don't want to predict if a patient in the training data has
  diabetes (because we already know this), we want to predict if a new
  patient has diabetes.
\end{itemize}

\end{frame}

\begin{frame}

\begin{center}\includegraphics[width=0.8\linewidth]{2StatLearn.1_files/figure-beamer/trainMSE-1} \end{center}

\textbf{Q}: Based on the training MSE - which model fits the data the
best?

Polynomial example: fitted order 1-20 polynomial when the truth is order
2. Left: one repetition, right: 100 repetitions of the training set.

\end{frame}

\begin{frame}

\begin{block}{Test MSE}

\begin{itemize}
\item
  Simple solution: estimate \(\hat{f}\) using the training data (maybe
  my minimizing the training MSE), but choose the \emph{best} model
  using a separate \emph{test set}.
\item
  \emph{Test MSE} for a set of \(n_0\) test observations
  \((x_{0j},y_{0j})\):
\end{itemize}

\[ \text{MSE}_{\text{test}}=\frac{1}{n_0}\sum_{j=1}^{n_0} (y_{0j}-\hat{f}(x_{0j}))^2\]

\begin{itemize}
\tightlist
\item
  Alternative notation: \[\text{Ave}(y_0-\hat{f}(x_0))^2\] (taking the
  average over all available test observations).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\textbf{Q:} What if we do not have access to test data?

\textbf{A:} In Module 5 we will look into using \emph{cross validation}
to mimic the use of a test set.

\textbf{Q:} But, can we instead just use the training data MSE to choose
a model? A low training error should also give a low test error?

\textbf{A:} Sadly no, if we use a flexible model we will look at several
cases where a low training error is a sign of overfitting, and will give
a high test error. So, the training error is not a good estimator for
the test error because it does not properly account for model
complexity.

\end{frame}

\begin{frame}

\begin{center}\includegraphics[width=0.9\linewidth]{2StatLearn.1_files/figure-beamer/traintestMSE-1} \end{center}

Polynomial example: fitted order 1-20 when the truth is order 2. Left:
one repetition, right: 100 repetitions for the testMSE.

\textbf{Q}: Based on the test MSE - which model fits the data the best?

\end{frame}

\begin{frame}

\begin{center}\includegraphics{2StatLearn.1_files/figure-beamer/unnamed-chunk-1-1} \end{center}

\textbf{A}: If choosing flexibility based on training MSE=poly20 wins,
if choose flexibility based on test MSE=poly 2 wins.

\end{frame}

\begin{frame}

\begin{block}{Test error vs.~training error}

\(~\)

Important observations:

\begin{itemize}
\item
  The test error seems to have a minimum (U-shape) in between the
  extremes.
\item
  The training error keeps going down.
\end{itemize}

\vspace{1.5cm} \centering
Why?

\end{block}

\end{frame}

\begin{frame}{The Bias-Variance trade-off}

\begin{itemize}
\item
  Assume we have fitted a \emph{regression} curve
  \(Y = f(x) + \varepsilon\) to our training data, which consist of
  independent observation pairs \(\{x_i, y_i\}\) for \(i=1,..,n\). (Yes,
  only one covariate \(x\).)
\item
  Assume that \(\varepsilon\) is an unobserved random variable that adds
  noise to the relationship between the response variable and the
  covariates (random error), with mean zero and constant variance
  \(\sigma^2\) for all values of \(x\).
\item
  \(\varepsilon\) is a substitute for all the unobserved variables that
  influence \(Y\).
\item
  The training data was used to estimate \(\hat{f}\).
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  We want to use \(\hat{f}\) to obtain the predicted response value
  \(\hat{f}(x_0)\) for an unseed test observation \((x_0,y_0)\).
\item
  The \emph{expected test mean squared error (MSE) at \(x_0\)} is
  defined as: \[\text{E}[y_0 - \hat{f}(x_0)]^2\]
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  Compare this to the test MSE for the polynomical example
  (\(\text{MSE}_{\text{test}}\)): The average is simply replaced by the
  \emph{theoretical version} (expected value).
\end{itemize}

\end{frame}

\begin{frame}

Using that \(y_0=f(x_0)+\varepsilon\), this expected test MSE can be
decomposed into three terms

\begin{align*}
& \text{E}[f(x_0)+\varepsilon - \hat{f}(x_0)]^2  \\
&= \text{E}[f(x_0)^2] +\text{E}[\varepsilon^2] + \underbrace{\text{E}[\hat{f}(x_0)^2]}_{=f(x_0)^2} -2\text{E}[f(x_0)\hat{f}(x_0)] \\
&= \text{Var}(f(x_0)) + \text{E}[f(x_0)^2] + \text{Var}(\varepsilon) + \text{Var}(\hat{f}(x_0)) +  \text{E}[\hat{f}(x_0)]^2 \\
& \qquad -2 f(x_0)\text{E}[\hat{f}(x_0)] \\
& =  \underbrace{\text{Var}(\varepsilon)}_{\text{Irreducible error}} + \underbrace{\text{Var}(\hat{f}(x_0))}_{\text{Variance of prediction}} + \underbrace{\left( f(x_0) - \text{E}[\hat{f}(x_0)] \right)^2}_{\text{Squared bias}}
\end{align*}

\textbf{Q}: what assumptions have we made in the derivation above?

\textbf{A}: classnotes.

\end{frame}

\begin{frame}

\[\text{E}[(y_0 - \hat{f}(x_0))^2]=\cdots=\text{Var}(\varepsilon) +  \text{Var}(\hat{f}(x_0))+[\text{Bias}(\hat{f}(x_0))]^2\]

\begin{itemize}
\item
  First term: irreducible error. This term cannot be reduced regardless
  how well our statistical model fits the data.
\item
  Second term: variance of the prediction at \(x_0\) or the expected
  deviation around the mean at \(x_0\). If the variance is high, there
  is large uncertainty associated with the prediction.
\item
  Third term: squared bias. The bias gives an estimate of how much the
  prediction differs from the true mean. If the bias is low the model
  gives a prediction which is close to the true value.
\end{itemize}

\end{frame}

\begin{frame}

Note: \[\text{E}[(y_0 - \hat{f}(x_0))^2]\]

is the \textbf{expected test MSE}. We can think of this as the average
test MSE we would obtain if we repeatedly estimated \(f\) using many
training sets (as we did in our example), and then tested this estimate
at \(x_0\).

However, if we also assume that \(X\) is a random variable, this is
really \(\text{E}[(Y - \hat{f}(x_0))^2 \mid X=x_0]\)

The \textbf{overall expected test MSE} can we then compute by averaging
the expected test MSE over all possible values of \(x_0\) (averaging
with respect to frequency in test set), or mathematically by the law of
total expectation \(\text{E} \{ \text{E}[(Y - \hat{f}(X))^2 \mid X]\}\)
(also sometimes referred to as the law of double expectations).

\end{frame}

\begin{frame}

\begin{block}{Polynomial example (cont.)}

\(Y=x^2\) is still the \emph{truth}.

\begin{center}\includegraphics[width=0.9\linewidth]{2StatLearn.1_files/figure-beamer/biasvarianceforx-1} \end{center}

For 4 different polynomial models (poly1,2,10 and 20), the squared bias,
variance, irreducible error and the total sum. Plots based on 100
simulations for the polynomial example.

\end{block}

\end{frame}

\begin{frame}

\begin{center}\includegraphics{2StatLearn.1_files/figure-beamer/biasvarianceforpoly-1} \end{center}

At 4 different values for \(x_0\), the squared bias, variance,
irreducible error and the total sum. Plots based on 100 simulations for
the polynomial example.

\end{frame}

\begin{frame}

\begin{center}\includegraphics[width=0.9\linewidth]{2StatLearn.1_files/figure-beamer/averageoverxs-1} \end{center}

Overall version (averaging over 61 gridpoints of x).

\end{frame}

\begin{frame}

\begin{block}{Choosing the best model}

\(~\)

When fitting a statistical model the aim is often to obtain \textbf{the
most predictive model}.

\begin{itemize}
\item
  \textbf{Training set}: The observations used to fit the statistical
  model \(\rightarrow\) Training error
\item
  \textbf{Test sample}: new observations which were not used when
  fitting the model \(\rightarrow\) Test error
\item
  Training error decreases for more complex models, but the test error
  has an \textbf{optimum}.
\item
  This trade-off in selecting a model with the right amount of
  complexity/flexibility is the \textbf{Bias-Variance trade-off}.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\includegraphics{../../ISLR/Figures/Chapter2/2.12.png}

\begin{itemize}
\tightlist
\item
  Inflexible models may lead to a poor fit (high bias)
\item
  Flexible (complex) models may provide more unbiased fits but may
  overfit the data (high variance)
\item
  The aim is to find the optimum.
\end{itemize}

\end{frame}

\begin{frame}{Classification (ev to be removed, with reference to see
Module 4!!)}

\vspace{2mm} How about model accuracy in classification? \vspace{2mm}

\textbf{Set-up:} Training observations
\(\{(x_1, y_1), ..., (x_n, y_n)\}\) where the response variable \(Y\) is
categorical, e.g \(Y \in \mathcal{C} = \{0, 1, ..., 9\}\) or
\(Y \in \mathcal{C} = \{dog, cat,... ,horse\}\).

\textbf{Aim: } To \emph{build} a classifier \(f(x)\) that assigns a
class label from \(\mathcal{C}\) to a future unlabelled observation
\(x\) and to asses the \emph{uncertainty} in this classification.

\textbf{Performance measure:} Most popular is the misclassification
error rate (training and test version).

\end{frame}

\begin{frame}

\begin{block}{Synthetic example}

\begin{itemize}
\item
  Simulate \(2\times 100\) observations from a bivariate normal
  distribution with mean vectors \(\mu_A = (1, 1)^T\),
  \(\mu_B = (3, 3)^T\), and covariance matrix
  \(\Sigma_A = \Sigma_B = \begin{pmatrix} 2\hspace{2mm} 0 \\ 0 \hspace{2mm} 2 \end{pmatrix}\).
\item
  Aim: Find a rule to classify a new observation to \(A\) or \(B\).
\end{itemize}

\begin{center}\includegraphics[width=0.55\linewidth]{2StatLearn.1_files/figure-beamer/knn-1} \end{center}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\tightlist
\item
  \textbf{Training error rate}: The proportion of mistakes that are made
  if we apply our estimator \(\hat{f}\) to the training observations,
  i.e. \(\hat{y}_i=\hat{f}(x_i)\)
  \[\frac{1}{n}\sum_{i=1}^n \text{I}(y_i \neq \hat{y}_i) \ ,\] with
  indicator function I, which is defined as:
\end{itemize}

\[\text{I}(a\neq\hat{a}) = \begin{cases} 1 \text{ if } a \neq \hat{a} \ , \\ 
0 \text{ else. } \end{cases}\]

\begin{itemize}
\item
  The indicator function counts the number of times our model has made a
  wrong classification. The training error rate is the fraction of
  misclassifications made on our training set.
\item
  A very low training error rate may imply overfitting.
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  \textbf{Test error rate}: The fraction of misclassifications when our
  model is applied on a test set
  \[\text{Ave}(I(y_0\neq \hat{y}_0)) \ ,\] where the average is over all
  the test observations \((x_0,y_0)\).
\item
  Again, this gives a better indication of the true performance of the
  classifier (than the training error).
\item
  We assume that a \emph{good} classifier is a classifier that has a
  \emph{low} test error.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Bayes classifier}

\begin{itemize}
\item
  The \emph{Bayes classifier assigns an observation to the most likely
  class}, given its predictor values.
\item
  Suppose we have a quantitative response value that can be a member in
  one of \(K\) classes
  \(\mathcal{C} = \{c_1, c_2, ..., c_k, ..., c_K\}\). Further, suppose
  these elements are numbered \(1, 2, ..., K\). The probability of that
  a new observation \(x_0\) belongs to class \(k\) is
  \[p_k(x_0) = \text{Pr}(Y=k | X=x_0), \quad k = 1, 2, ... K.\] This is
  the conditional class probability: the probability that \(Y=k\) given
  the observation \(x_0\).
\item
  Two-class example for two groups \(\{A, B\}\). A new observation
  \(x_0\) will be classified to \(A\) if
  \(\text{Pr}(Y=A | X=x_0) > 0.5\) and to class \(B\) otherwise.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Properties of the Bayes classifier}

\begin{itemize}
\item
  It has the \emph{smallest test error rate}.
\item
  The class boundaries using the Bayes classifier is called the
  \emph{Bayes decision boundary}.
\item
  The overall Bayes error rate is given as
  \[1-\text{E}(\text{max} \text{Pr}(Y=j\mid X))\] where the expectation
  is over \(X\).
\item
  The Bayes error rate is comparable to the \emph{irreducible error} in
  the regression setting.
\item
  Caveat: we never (or very seldom) know the conditional distribution of
  \(Y\) given \(X\) for real data \(\rightarrow\) The \(K\)-nearest
  neighbor classifier estimates this conditional distribution and then
  classifies a new observation based on this estimated probability.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{K-nearest neighbour classifier}

The \(K\)-nearest neighbour classifier (KNN) works in the following way:

\begin{itemize}
\tightlist
\item
  Given a new observation \(x_0\) it searches for the \(K\) points in
  our training data that are closest to it (Euclidean distance).
\item
  These points make up the neighborhood of \(x_0\), \(\mathcal{N}_0\).
\item
  The point \(x_0\) is classified by taking a majority vote of the
  neighbors.
\item
  That means that \(x_0\) is classified to the most occurring class
  among its neighbors
  \[\text{Pr}(Y=j | X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j).\]
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

We return to our synthetic data with \(X_1\) and \(X_2\) and two classes
\(A\) and \(B\):

\begin{itemize}
\tightlist
\item
  Assume we have a new observation \(X_0 = (x_{01}, x_{02})^T\) which we
  want to classify as belonging to the class \(A\) or \(B\).
\item
  To illustrate this problem we fit the \(K\)-nearest neighbor
  classifier to our simulated data set with \(K = 1, 3, 10\) and
  \(150\).
\end{itemize}

The small colored dots show the predicted classes for an evenly-spaced
grid. The lines show the decision boundaries. If our new observation
falls into the region within the red decision boundary, it will be
classified as \(A\). If it falls into the region within the green
decision boundary, it will be classified as \(B\).

\end{frame}

\begin{frame}

\includegraphics{2StatLearn.1_files/figure-beamer/unnamed-chunk-2-1.pdf}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  The choice of \(K\) has a big influence on the result of our
  classification. For \(K=1\) the classification is made to the same
  class as the one nearest neighbor. As \(K\) gets very large, the
  decision boundary tends towards a straight line (which is the Bayes
  boundary in this set-up).
\item
  To find the optimal value of \(K\) the typical procedure is to try
  different values of \(K\) and then test the predictive power of the
  different classifiers, for example by cross-validation, which will be
  discussed in M5.
\item
  After trying all choices for \(K\) between 1 and 50, a few choices of
  \(K\) gave the smallest misclassification error rate, estimating by
  leave-one out cross-validation (see M5). The smallest error rate is
  equal to 0.165 (16.5\% misclassification).
\end{itemize}

\end{frame}

\begin{frame}

\begin{center}\includegraphics{2StatLearn.1_files/figure-beamer/knnerror-1} \end{center}

\end{frame}

\begin{frame}

\(\rightarrow\) Bias-variance trade-off in a classification setting:

\begin{itemize}
\item
  A too low value of \(K\) will give a very flexible classifier (with
  high variance and low bias) which will fit the training set too well
  (it will overfit) and make poor predictions for new observations.
\item
  Choosing a high value for \(K\) makes the classifier loose its
  flexibility and the classifier will have low variance but high bias.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{The curse of dimensionality}

\begin{itemize}
\item
  The nearest neighbor classifier can be quite good if the number of
  predictor \(p\) is small and the number of observations \(n\) is
  large. We need enough close neighbors to make a good classification.
\item
  The effectiveness of the KNN classifier falls quickly when the
  dimension of the preditor space is high.
\item
  Why? Because the nearest neighbors tend to be far away in high
  dimensions and the method no longer is local. This is referred to as
  the \emph{curse of dimensionality}.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What was important in Part A?}

\begin{itemize}
\tightlist
\item
  prediction vs.~interpretation (inference)
\item
  supervised vs.~unsupervised methods
\item
  classification vs.~regression
\item
  parametric vs.~non-parametric methods
\item
  flexibility vs.~interpretation
\item
  under- and overfitting
\item
  quadratic and 0/1 loss functions
\item
  training and test MSE and misclassification error
\item
  bias-variance trade off
\item
  Bayes classifier and KNN-classifier
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]{ R packages}

If you want to look at the .Rmd file and \texttt{knit} it, you need to
first install the following packages (only once).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"knitr"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"kableExtra"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"rmarkdown"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"ggpubr"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"reshape2"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"ElemStatLearn"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"GGally"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"class"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"mvtnorm"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"MASS"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"car"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"faraway"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"reshape"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}{Acknowledgements}

Thanks to Julia Debik for contributing to this module page.

\end{frame}

\end{document}
