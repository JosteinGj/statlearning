---
title: 'Module 9: Recommended Exercises'
author: Martina Hall, Michail Spitieris, Stefanie Muff, Department of Mathematical
  Sciences, NTNU
date: "March 12, 2020"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
subtitle: TMA4268 Statistical Learning V2020
bibliography: refs.bib
urlcolor: blue
---

```{r setup, include=FALSE}
showsolA<-TRUE
showsolB<-TRUE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=5,fig.height=4.5,fig.align = "center",out.width='70%')
whichformat="html"
```



---

## Problem 1

Work through the lab in Section 9.6.1 of the course book.

## Problem 2 (Book Ex.2)

We have seen that in $p = 2$ dimensions, a linear decision boundary takes the form $\beta_0 +\beta_1 X_1 + \beta_2 X_2 = 0$. We now investigate a non-linear decision boundary.

a) Sketch the curve
$$(1 + X_1)^2 + (2 - X_2)^2 = 4.$$

b) On your sketch, indicate the set of points for which $$(1+X_1)^2 +(2-X_2)^2 >4,$$
as well as the set of points for which $$(1+X_1)^2 +(2-X_2)^2 \leq 4.$$

c) Suppose that a classifier assigns an observation to the blue class if
$$(1+X_1)^2 +(2-X_2)^2 >4,$$
and to the red class otherwise. To what class is the observation
$(0, 0)$ classified? $(-1,1)?\quad (2,2)? \quad (3,8)?$

d) Argue that while the decision boundary in (c) is not linear in
terms of $X_1$ and $X_2$, it is linear in terms of $X_1, X_1^2, X_2,$ and 
$X_2^2.$


# Problem 3

The SVM is an extension of the support vector classifier, by enlarging the feature space using Kernels. The polynomial Kernel is a popular choice and has the following form


$$K({\boldsymbol x}_i,{\boldsymbol x}_i')=(1+\sum_{j=1}^p x_{ij} x_{i'j'})^d \ $$
Show that for a feature space with inputs $X_1$ and $X_2$ and for degree $d=2,$ the above kernel can be represented as the inner product 
$$K({\boldsymbol x}_i,{\boldsymbol x}_i')=\langle h(x),h(x')\rangle \ ,$$ 
where $h(x)$ is a 6-dimensional transformation function in an enlarged space.

## Problem 4 

This problem involves plotting of decision boundaries for different kernels and it's taken from [Lab video](https://www.youtube.com/watch?v=qhyyufR0930&list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o&index=5).

<!--
@Michail: At the moment this problem is a very strange one. Students only need to copy-paste, so if you want to keep it, then please revise it in a way that students can do something. But now it's just a collection of code and unclear what the learning outcome should be here. Also, I don't want that much R-code in the exercise sheet (most of the code and figures produced from it should be given in the solutions).

* Go back and read in the `forest1` data (is located in the same place as `forest2`) and run the `svm` with a very high value for `cost`. The `forest1` is a separable problem.

* Linear version of SVM: Making nicer plots for SVM from [Lab video](https://www.youtube.com/watch?v=qhyyufR0930&list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o&index=5). Go through the code and see what is happening (and see the video if you want more explanation).
-->








```{r,eval=TRUE, echo=FALSE,out.width='50%'}
# code taken from video by Trevor Hastie linked above
# fake data
set.seed(10111)
x=matrix(rnorm(40),20,2)
y=rep(c(-1,1),c(10,10))
x[y==1,]=x[y==1,]+1
plot(x,col=y+3,pch=19, xlab = expression(X[1]), ylab = expression(X[2]))
dat=data.frame(x,y=as.factor(y))
```

(a) Plot the decision boundary of the `svmfit` model by using the function `make.grid`. Hint: Use the predict function for the grid points and then plot the predicted values {-1,1} with different colors.  

```{r}
library(e1071)
svmfit=svm(y~.,data=dat, kernel="linear",cost=10,scale=FALSE)

# grid for plotting
make.grid = function(x, n =75){
  # takes as input the data matrix x
  # and number of grid points n in each direction
  # the default value will generate a 75x75 grid
  
  grange = apply(x,2,range) # range for x1 and x2
  x1 = seq(from = grange[1,1], to = grange[2,1], 
           length.out = n) # sequence from the lowest to the upper value of x1
  x2 = seq(from = grange[1,2], to = grange[2,2], 
           length.out = n) # sequence from the lowest to the upper value of x2
  expand.grid(X1=x1, X2=x2) #create a uniform grid according to x1 and x2 values
}
```

(b) On the same plot add the training points and indicate the support vectors.

(c) The solutions to the SVM optimization problem is given by 

$$
\hat{\beta}= \sum_{i\in \mathcal{S}} \hat\alpha_i y_i x_i \ ,
$$
where $S$ is the set of the support vectors. From the `svm()` function we cannot extract $\hat \beta$, but instead we have access to $\text{coef}_i = \hat\alpha_i y_i$, and $\hat\beta_0$ is given as $rho.$ For more details see 
[here](https://cran.r-project.org/web/packages/e1071/vignettes/svminternals.pdf).

Calculate the coeficients $\hat\beta_0, \hat\beta_1$ and $\hat\beta_2.$ Then add the decisision boundary and the margins using the function `abline()` on the plot from (b). 


(d) Now we fit an svm model with radial kernel to the following data taken from @ESL:

```{r,eval=TRUE, echo=FALSE,out.width='55%'}
load(url("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda"))
#names(ESL.mixture)
rm(x,y)
attach(ESL.mixture)
plot(x,col=y+1, pch=19, xlab = expression(X[1]), ylab = expression(X[2]))
dat=data.frame(y=factor(y),x)
```


```{r}
fit=svm(factor(y)~.,data=dat,scale=FALSE,kernel="radial",cost=5)
```

Using the same idea as in (a) plot the non-linear decision boundary, and add the training points. Furthermore if you want to create the decision boundary curve you can use the argument `decision.values=TRUE` in the function predict, and then you can plot it by using the `contour()` function.



# Problem 5 - optional (Book Ex. 7) 

This problem involves the OJ data set which is part of the ISLR package.
(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

(b) Fit a support vector classifier to the training data using `cost=0.01`, with Purchase as the response and the other variables as predictors. Use the `summary()` function to produce summary statistics, and describe the results obtained.

(c) What are the training and test error rates?

(d) Use the `tune()` function to select an optimal cost. Consider values in the range 0.01 to 10.

(e) Compute the training and test error rates using this new value
for cost.

(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for `gamma`.

(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set `degree=2`.

(h) Overall, which approach seems to give the best results on this data?


